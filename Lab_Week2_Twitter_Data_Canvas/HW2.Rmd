---
title: "Homework 2"
author: "Neal Marquez"
date: "October 14, 2017"
output: html_document
---


```{r warning=FALSE,message=FALSE,error=FALSE}
rm(list=ls())
setwd("~/Documents/datascipop/Lab_Week2_Twitter_Data_Canvas/")

library(ROAuth)
library(streamR)
library(twitteR)
library(stringr)
library(plyr)
library(dplyr)
library(rjson)
library(pander)
source('functions_by_pablobarbera.R')
source('./sentiment.r')

# run only on initial run through

# load(file="./credentials/secretkeys.Rdata")
# load("./credentials/twitCred.Rdata")
# 
# setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
# 
# sampleStream("data/tweets_hw.json", timeout = 60, 
#              oauth = twitCred)

```

1) Find the 10 hashtags that are trending at the time you are working on your homework.

```{R}
tweets <- parseTweets("data/tweets_hw.json", verbose = TRUE)
pander(getCommonHashtags(tweets$text, n=10))
```

2) Choose 2 of those hashtags and collect 500 tweets for each of them.

```{R}
# Nothing is run here after we already have the tweets

# tweets_cc <- searchTwitter("@CamilaCabello", n=500)
# tweets_L<- searchTwitter("@L", n=500)
# #transform your data into a data frame 
# data_cc <- twListToDF(tweets_cc)
# data_L <- twListToDF(tweets_L)
# 
# head(data_cc)
# 
# 
# # write the data to disk 
# write.csv(data_cc,"data/data_cc.csv")
# write.csv(data_L,"data/data_L.csv")
```

3) Evaluate the average and standard deviation for sentiments associated to the two hashtags using the lexicon that we used in class.

```{R}
# import positive and negative words
pos <- readLines("opinion_lexicon/positive_words.txt")
neg <- readLines("opinion_lexicon/negative_words.txt")

data_cc <- read.csv("./data/data_cc.csv")
data_L <- read.csv("./data/data_L.csv")

scores_cc <- score.sentiment(data_cc$text,pos, neg)$score
scores_L <- score.sentiment(data_L$text,pos, neg)$score

sent_scores <- data.frame(score=c(scores_cc, scores_L),
                          tag=rep(as.factor(c("CamilaCabello", "L")), each=500))

score_params <- sent_scores %>% dplyr::group_by(tag) %>%
    dplyr::summarize(mean_score=mean(score), sd_score=sd(score))
pander(score_params)
```

4) Evaluate the sample size needed to achieve a confidence interval with a width of 0.01 or less, for the average sentiments for each of your two hashtags.

```{R}
N_df <- score_params %>% 
    mutate(N_needed=(sd_score * ((1.96 * 2) / .01))**2) %>%
    select(tag, N_needed)

pander(N_df)
```

5) In the folder called "00", consider the json files that you can extract (decompress) from "00.json.bz2", "01.json.bz2", "02.json.bz2". For each of those 3 files write an R script to extract a list of user IDs for users who have at least 2,000 followers. Briefly count the number of unique users who match those criteria in each of the three files, and attach your list of user IDs.

```{R}
bzfiles <- paste0("./01/00/", c("00", "01", "02"), ".json.bz2")

tweet_df <- lapply(bzfiles, parseTweets)

top_follower <- do.call(rbind, lapply(1:3, function(i)
    tweet_df[[i]] %>% select(user_id_str, followers_count) 
    %>% filter(followers_count > 2000)
    %>% unique %>% mutate(dataset=i))) %>% arrange(desc(followers_count))

pander(top_follower %>% group_by(dataset) %>% summarize(count=n()))
```

```{R}
pander(top_follower)
```